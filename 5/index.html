<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project 5: Fun with Diffusion Models</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0 auto;
            padding: 40px;
            max-width: 800px;
        }
        h1, h2, h3, h4, h5 {
            color: #333;
            text-align: center;
        }
        p {
            text-align: left;
        }
        img {
            max-width: 100%;
            height: auto;
            margin: 10px 0;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
        .image-pair, .image-trio {
            display: flex;
            justify-content: center;
            gap: 10px;
        }
        .image-pair img, .image-trio img {
            width: 48%;
        }
        .image-trio img {
            width: 32%;
        }
        .small-images img {
            width: 25%;
        }
        .large-images img {
            width: 60%;
            margin-bottom: 10px;
        }
        .labeled-pair {
            text-align: center;
        }
        .labeled-pair small {
            display: block;
            margin-top: 5px;
            color: #666;
        }
        code {
            background-color: #f5f5f5;
            padding: 2px 4px;
            font-family: monospace;
        }
        a {
            color: blue;
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>Project 5: Fun with Diffusion Models</h1>
    <h3>Albert Wang</h3>
    <p>In this project, I implemented and deployed diffusion models for image generation.</p>

    <h2>Part A: The Power of Diffusion Models</h2>

    <h3>Part 0: Setup</h3>
    <p>First, we load the Deepfloyd IF diffusion model from HuggingFace. Deepfloyd was trained as a text-to-image model, so we also load some precomputed text embeddings. These are generated with a seed of 42. I also experimented with different inference steps (50 instead of 20). Here are a few sample images generated by those prompts:</p>
    <img src="media/part0.jpg" alt="Part 0 Sample Images">

    <h3>Part 1.1: Implementing the Forward Process</h3>
    <p>First, we implement the forward process of diffusion, which takes a clean image and adds noise to it. The process runs these equations:</p>
    <img src="media/EQ1.png" alt="Forward Process Equations">
    <p>Here are images of the campanile at noise levels 250, 500, and 750:</p>
    <img src="media/noise.jpg" alt="Noisy Images">

    <h3>Part 1.2: Classical Denoising</h3>
    <p>Next, we will try to use Gaussian blur filtering to remove the noise. We can see that it is not very effective.</p>
    <img src="media/noisegauss.jpg" alt="Gaussian Blur Denoising">

    <h3>Part 1.3: One-Step Denoising</h3>
    <p>Next, we will use a pretrained diffusion model to denoise the image. For each of the three noisy images from 1.2, we denoised the image using the U-Net. This model predicted the noise in the image and attempts to remove it. Compared to Gaussian blur, this approach yielded much better results, though it struggled at higher noise levels.</p>
    <img src="media/onestep.jpg" alt="One-Step Denoising Results">

    <h3>Part 1.4: Iterative Denoising</h3>
    <p>Single-step denoising was effective, but we take it further by implementing an iterative denoising process. Starting with a highly noisy image, I gradually reduced the noise over multiple steps.</p>
    <p>We use this equation:</p>
    <img src="media/eq2.jpg" alt="Iterative Denoising Equation">
    <p>where:</p>
    <img src="media/eq3.jpg" alt="Noise Scaling Coefficients">
    <p>At each step, we remove some noise based on specific calculations using noise scaling coefficients. We denoise from t = 990 to t = 0 with a stride of 30. The results were significantly better, especially for heavily noised images.</p>
    <img src="media/1.4part1.png" alt="Iterative Denoising Part 1">
    <img src="media/1.4part2.png" alt="Iterative Denoising Part 2">

    <h3>Part 1.5: Diffusion Model Sampling</h3>
    <p>After understanding denoising, I used the diffusion model to generate images from scratch. Starting with pure noise, I iteratively denoised to create "high-quality photos". Here are the results:</p>
    <img src="media/hq.png" alt="High-Quality Photos from Diffusion Sampling">

    <h3>Part 1.6: Classifier-Free Guidance (CFG)</h3>
    <p>Next, we use classifier-free guidance (CFG), which combines conditional and unconditional noise estimates to improve image quality. By emphasizing relevant details while retaining some randomness, CFG enhanced the coherence and structure of the generated images. Intuitively, it can be thought of as noise moving more in the direction of the condition given in the model. We combine CFG on top of iterative denoising, so the noise is progressively refined until we get a structured image. Here are the results:</p>
    <img src="media/cfg.png" alt="Classifier-Free Guidance Results">

    <h3>Part 1.7: Image-to-Image Translation</h3>
    <p>Recall in part 1.4 we took a real image, added noise to it, and denoised it. This essentially caused us to make edits to those images. The more noise added, the more the diffusion model "hallucinated" new things. Here, we do a similar process, this time using CFG, following the <a href="https://sde-image-editing.github.io/" target="_blank">SDEdit</a> algorithm. We run the forward process to get a noisy test image, and then run <code>iterative_denoise_cfg</code> using a starting index of [1, 3, 5, 7, 10, 20] steps. Here are the results:</p>
    <img src="media/series1.png" alt="Image-to-Image Translation Series 1">
    <img src="media/series2.png" alt="Image-to-Image Translation Series 2">
    <img src="media/series3.png" alt="Image-to-Image Translation Series 3">

    <h4>1.7.1 Edits and Variations</h4>
    <p>Next, I tried this on some web images and some hand-drawn images:</p>
    <img src="media/corgiseries.jpg" alt="Corgi Series">
    <img src="media/pandaseries.jpg" alt="Panda Series">
    <img src="media/treeseries.jpg" alt="Tree Series">

    <h4>1.7.2 Inpainting</h4>
    <p>Using binary masks, I inpainted missing regions in images by combining the forward process and diffusion denoising loop. At every step, after obtaining <code>x_t</code>, we ensure that <code>x_t</code> has the same pixels as <code>x_{origin}</code> in areas that are not masked out. This technique fills in the gaps from the mask while preserving unmasked areas. Here are the results:</p>
    <div class="large-images">
        <img src="media/campanile inpaint.jpg" alt="Campanile Inpaint">
        <img src="media/goku inpaint.jpg" alt="Goku Inpaint">
        <img src="media/dino inpaint.jpg" alt="Dino Inpaint">
    </div>

    <h4>1.7.3 Text-Conditional Image-to-Image Translation</h4>
    <p>Next, we do the same thing as SDEdit, but we guide the projection with different text prompts that are transformed into the input images:</p>
    <img src="media/rocket to campanile.jpg" alt="Rocket to Campanile">
    <img src="media/dog to pickle.jpg" alt="Dog to Pickle">
    <img src="media/hat to cloud.jpg" alt="Hat to Cloud">

    <h3>Part 1.8: Visual Anagrams</h3>
    <p>Next, we create images that flip between two interpretations depending on orientation. For example, one image shows "an oil painting of people around a campfire" but reveals "an oil painting of an old man" when flipped upside down.</p>
    <p>To do this, we will denoise an image <code>x_t</code> at each step normally with the prompt "an oil painting of an old man" to obtain noise estimate <code>ϵ₁</code>. But at the same time, we will flip the image upside down and denoise with the prompt "an oil painting of people around a campfire" to get noise estimate <code>ϵ₂</code>. We can flip <code>ϵ₂</code> back to make it right-side up and average the two noise estimates. We can then perform a reverse/denoising diffusion step with the averaged noise estimate. The algorithm is as follows:</p>
    <img src="media/eqn4.jpg" alt="Visual Anagrams Algorithm">
    <p>Here are the results:</p>
    <div class="labeled-pair">
        <div class="image-pair">
            <img src="media/old man.png" alt="Old Man">
            <img src="media/campfire.png" alt="Campfire">
        </div>
        <small>Old Man / Campfire</small>
    </div>
    <div class="labeled-pair">
        <div class="image-pair">
            <img src="media/waterfall.png" alt="Waterfall">
            <img src="media/skull.png" alt="Skull">
        </div>
        <small>Waterfall / Skull</small>
    </div>
    <div class="labeled-pair">
        <div class="image-pair">
            <img src="media/dog.png" alt="Dog">
            <img src="media/man with hat.png" alt="Man with Hat">
        </div>
        <small>Dog / Man with Hat</small>
    </div>

    <h3>Part 1.9: Hybrid Images</h3>
    <p>Just like in project 2, we can also create hybrid images using frequency-based blending. We create a composite noise estimate by estimating the noise with two different text prompts and then combining low frequencies from one noise estimate with high frequencies of the other. The algorithm is as follows:</p>
    <img src="media/eqn5.jpg" alt="Hybrid Images Algorithm">
    <p>Here are the results:</p>
    <div class="large-images">
        <div class="labeled-pair">
            <img src="media/skullwaterfall.png" alt="Skull and Waterfall">
            <small>Skull / Waterfall</small>
        </div>
        <div class="labeled-pair">
            <img src="media/skulldog.png" alt="Skull and Dog">
            <small>Dog / Skull</small>
        </div>
        <div class="labeled-pair">
            <img src="media/manwaterfall.png" alt="Man and Waterfall">
            <small>Old Man / Waterfall</small>
        </div>
    </div>


    <h2>Part 5B: Diffusion Models From Scratch</h2>
    <p>In this part, I will train my own diffusion model on MNIST.</p>

    <h3>Part 1: Training a Single-Step Denoising Net</h3>
    <p>First, we start by training a one-step denoiser. In other words, this model will map a noisy image to a clean image by optimizing over L2 Loss.</p>

    <h4>1.1: Implementing the UNet</h4>
    <p>To implement the UNet, we use downsampling and upsampling blocks with skip connections. This diagram describes the procedure:</p>
    <img src="media/diag1.png" alt="UNet Architecture Diagram">
    <p>It uses a number of standard tensor operations defined as follows:</p>
    <img src="media/diag2.png" alt="Standard Tensor Operations">
    <p>Our UNet architecture utilizes convolutional layers with GELU activation functions in the encoder for downsampling and transposed convolutional layers in the decoder for upsampling. Skip connections are integrated between the encoder and decoder layers. The model is trained on image pairs using MSE loss and optimized with the Adam optimizer.</p>

    <h4>1.2: Using the UNet to Train a Denoiser</h4>
    <p>Here is a visualization of the different noising processes over sigma values of [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]:</p>
    <img src="media/output1.png" alt="Noising Process Visualization">

    <h5>1.2.1: Training</h5>
    <p>Next, we train on noisy images with sigma = 0.5 and dynamically add image batches using a batch size of 256 for 5 epochs. We define our model with hidden dimension 128 and use a fixed learning rate of 0.0001. This yields the following loss curve:</p>
    <img src="media/loss1.png" alt="Training Loss Curve">
    <p>We visualize the denoised results on the test set for the model after the first and fifth epoch:</p>
    <div class="image-pair">
        <img src="media/output2.png" alt="Denoised Results After Epoch 1">
        <img src="media/output3.png" alt="Denoised Results After Epoch 5">
    </div>

    <h5>1.2.2: Out-of-Distribution Testing</h5>
    <p>Our model was trained on MNIST digits noised with sigma = 0.5. Next, we test the performance of our denoiser on sigmas = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]. Here are the results:</p>
    <img src="media/output4.png" alt="Out-of-Distribution Testing Results">

    <h3>Part 2: Training a Diffusion Model</h3>

    <h4>2.1: Adding Time Conditioning to UNet</h4>
    <p>Next, we train a time-conditioned UNet for iterative denoising within a DDPM framework. Instead of predicting clean images, the model predicts noise, using a variance schedule that increases noise from 0.0001 to 0.02 over 300 timesteps. By conditioning the UNet on the timestep, a single model can handle varying noise levels during the diffusion process. The training objective minimizes the mean squared error between the predicted and actual noise at randomly sampled timesteps.</p>
    <img src="media/diag3.png" alt="Time-Conditioned UNet Architecture">
    <img src="media/diag4.png" alt="Variance Schedule and Loss Function">

    <h4>2.2: Training the UNet</h4>
    <p>We use batch size 128 for 20 epochs, learning rate = 0.001, and hidden dimension of 64. We follow this training algorithm:</p>
    <img src="media/diag5.png" alt="Training Algorithm">
    <p>This generates the following time-conditioned UNet training loss graph:</p>
    <img src="media/loss2.png" alt="Time-Conditioned UNet Training Loss">

    <h4>2.3: Sampling from the Diffusion Model</h4>
    <p>Next, we sample from our diffusion model. This yields these results for epochs 1, 5, and 20:</p>
    <img src="media/epoch1.png" alt="Samples at Epoch 1">
    <img src="media/epoch5.png" alt="Samples at Epoch 5">
    <img src="media/epoch20.png" alt="Samples at Epoch 20">
 

    <h4>2.4: Adding Class-Conditioning to UNet</h4>
    <p>Next, to add class-conditioning to our model, we add two additional FCBlocks for processing class as well as time, where we use a one-hot encoding for class. Training for this section will be the same as time-only, with the only difference being the conditioning vector c and doing unconditional generation periodically. We follow this algorithm:</p>
    <img src="media/diag6.png" alt="Class-Conditioned Training Algorithm">
    <p>Here is the class-conditioned UNet training loss graph:</p>
    <img src="media/loss3.png" alt="Class-Conditioned UNet Training Loss">

    <h4>2.5: Sampling from the Class-Conditioned UNet</h4>
    <p>Here are the results of sampling from epochs 5 and 20 from the class-conditioned UNet:</p>
    <div class="labeled-pair">
        <img src="media/epoch5.jpg" alt="Class-Conditioned Samples at Epoch 5">
        <small>Epoch 5</small>
    </div>
    <div class="labeled-pair">
        <img src="media/epoch20.jpg" alt="Class-Conditioned Samples at Epoch 20">
        <small>Epoch 20</small>
    </div>

</body>
</html>
